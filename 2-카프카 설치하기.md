---
title: 2-카프카 설치하기
tags:
  - 도서/카프카-핵심-가이드
---
## 환경 설정

### 운영체제 선택하기

- 아파치 카프카는 다양한 운영체제에서 실행이 가능한 자바 애플리케이션이다.
- 카프카는 윈도우, macOS, 리눅스 등 다양한 운영체제에서 실행이 가능하지만, 대체로 리눅스가 권장된다.

### 주키퍼 설치하기

- 아파치 카프카는 카프카 클러스터의 메타데이터와 컨슈머 클라이언트에 대한 정보를 저장하기 위해 아파치 주키퍼를 사용한다.
- 주키퍼는 설정 정보 관리 이름 부여, 분산 동기화, 그룹 서비스를 제공하는 중앙화된 서비스이다.
	- ![](assets/Pasted%20image%2020250411213038.png)
- 독립 실행 서버이 가능하다.
- 주키퍼는 고가용성을 보장하기 위해 앙상블이라 불리는 클러스터 단위로 작동하도록 설계되었다.
- 주키퍼가 사용하는 부하 분산 알고리즘 때문에 앙상블은 홀수 개의 서버를 가지는 것이 권장된다.
	- 주키퍼가 요청에 응답하려면 앙상블 멤버(쿼럼이라고 부른다)의 과반 이상이 작동하고 있어야 하기 때문이다.
- 주키퍼 앙상블을 구성할 때는 5개의 노드 크기를 고려하자.
	- 앙상블 설정을 변경할 수 있게 하려면, 한 번에 한 대의 노드를 정지시켰다가 설정을 변경한 뒤 다시 시작해야 한다.
	- 만약 앙상블이 2대 이상의 노드 정지를 받아낼 수 없다면, 정비 작업을 수행하는 데는 위험이 따를 수밖에 없다.
	- 그렇다고 해서 9대 이상의 노드를 사용하는 것도 권장하는 않는데,합의 프로토콜 특성상 성능이 내려가기 시작할 수 있기 때문이다.
	- 또한, 클라이언트 연결이 너무 많아서 5대 혹은 7대의 노드가 부하를 감당하기에 모자란다는 생각이 들면 옵저버 노드를 추가함으로써 읽기 전용 트래픽을 분산시킬 수 있도록 해 보자.
- 주키퍼 서버를 앙상블로 구성하기 위해 필요한 2가지
	- 각 서버는 공통된 설정 파일을 사용해야 한다.
	- 각 서버는 데이터 디렉토리에 자신의 ID 번호를 지정하는 myid 파일을 가지고 있어야 한다.
- 설정 파일
	- ![](assets/Pasted%20image%2020250411214809.png)
	- `dataDir`: 데이터 디렉토리 경로. 해당 경로에 자신의 ID 번호를 지정하는 myid 파일을 가지고 있어야 한다.
	- `initLimit`: 팔로워가 리더와 연결할 수 있는 최대 시간. 이 시간 동안 리더와 연결을 못 하면 초기화에 실패한다.
	- `syncLimit`: 팔로워가 리더와 연결할 수 있는 최대 시간. 이 시간 동안 리더와 연결을 못 하면 동기화가 풀린다.
	- `initLimit`과 `syncLimit` 값은 `tickTime` 단위로 정의되는데, 위 예시에서 초기화 제한 시간은 20\*2000밀리초, 즉 40초가 된다.
	- 설정 파일에는 앙상블 안의 모든 서버 내역을 정의한다.
		- `server.{X}={hostname}:{peerPort}:{leaderPort}` 꼴로 정의된다.
		- `X`: 서버의 ID. 정숫값이어야 하지만, 0부터 시작할 필요도 없고 순차적으로 부여될 필요도 없다.
		- `hostname`: 서버의 호스트명 또는 IP 주소
		- `peerPort`: 앙상블 안의 서버들이 서로 통신할 때 사용되는 TCP 포트 번호
		- `leaderPort`: 리더를 선출하는 데 사용되는 TCP 포트 번호
	- `clientPort`: 클라이언트가 앙상블에 연겨할 때 사용할 수 있는 포트 번호

## 카프카 브로커 설치하기

- 자바와 주키퍼가 설정되었다면 아파치 카프카를 설치할 수 있다.

## 브로커 설정하기

### 핵심 브로커 매개변수

- 어떤 환경에서 카프카를 설치하던 간에 살펴봐야만 하는 브로커 설정 매개변수
- `broker.id`
	- 카프카 브로커의 정숫값 식별자.
	- 기본값은 0이지만, 어떤 값도 될 수 있다.
	- 이 정숫값이 클러스터 안의 각 브로커별로 전두 달라야 한다.
- `listeners`
	- 외부와 통신할 때 어떤 주소로 열려있을지 설정한다.
	- `{프로토콜}://{호스트 이름}:{포트}`의 형태로 정의된다.
		- 예시) `PLAINTEXT://localhost:9092,SSL://:9091`
	- 호스트 이름을 0.0.0.0으로 잡아줄 경우 모든 네트워크 인터페이스로부터 연결을 받게되며, 이 값을 비워 주면 기본 인터페이스에 대해서만 연결을 받게 된다.
	- 1024 미만의 포트 번호를 사용할 경우 루트 권한으로 카프카를 실행시켜야 한다.
	- 만약 리스너 이름이 일반적인 보안 프로토콜이 아니라면, 반드시 `listener.security.protocol.map` 설정을 잡아주어야 한다.
- `zookeeper.connect`
	- 브로커의 메타데이터가 저장되는 주키퍼의 위치
	- 세미콜론으로 연결된 `{호스트 이름}:{포트}/{경로}`의 목록 형시으로 지정할 수 있다.
	- `호스트 이름`: 주키퍼 서버의 호스트 일므이나 IP 주소
	- `포트`: 주키퍼의 클라이언트 포트 번호
	- `/{경로}`: 선택 사항. 카프카 클러스터의 chroot 환경으로 사용될 주키퍼 경로. 지정하지 않으면 루트 디렉토리가 사용된다.
		- 대체로 카프카 클러스터에는 chroot 경로를 지정하는 것이 좋다.
		- 다른 애플리케이션과 충돌할 일 없이 주키퍼 앙상블을 공유해서 사용할 수 있기 때문이다.
	- 같은 앙상블에 속하는 다수의 주키퍼 서버를 지정하는것이 좋다.
		- 특정 서버에 장애가 생기더라도 카프카 브로커가 같은 주키퍼의 달느 서버에 연결할 수 있기 때문이다.
- `log.dirs`
	- 카프카는 모든 메시지를 로그 세그먼트 단위로 묶어서 `log.dir` 설정에 저장된 디스크 디렉토리에 저장한다.
	- 다수의 디렉토리를 지정하고자 할 경우, `log.dirs`를 사용하는 것이 좋다.
	- `log.dirs`가 설정되어 있지 않을 경우, `log.dir`가 사용된다.
	- `log.dirs`는 쉼표로 구분된 로컬 시스템 경로의 목록이다.
	- 1개 이상의 경로가 지정되었을 경우, 브로커는 가장 적은 수의 파티션이 저장된 디렉토리에 새 파티션을 저장할 것이다.
		- 같은 파티션에 속하는 로그 세그먼트는 동일한 경로에 저장된다.
	- 사용된 디스크 용량 기준이 아닌 저장된 파티션 수 기준으로 새 파티션의 저장 위치를 배정한다는 점에서, 다수의 디렉토리에 대해 균등한 양의 데이터가 저장되지 않는다.
- `num.recovery.threads.per.data.dir`
	- 카프카는 스레드 풀을 사용해서 로그 세그먼트를 관리한다.
	- 이 스레드 풀은 아래와 같은 작업을 수행한다.
		- 브로커가 정상적으로 시작되었을 때, 각 파티셔의 로그 세그먼트 파일을 연다.
		- 브로커가 장애 발생 후 다시 시작되었을 때, 각  파티션의 로그 세그먼트를 검사하고 잘못된 부분은 삭제한다.
		- 브로커가 종료할 때, 로그 세그먼트를 정상적으로 닫는다.
	- 기본적으로, 하나의 로그 디렉토리에 대해 하나의 스레드만 사용된다.
	- 이 스레드들은 브로커가 시작될 때와 종료될 때만 사용되기 때문에 작업을 병렬화하기 위해서는 많은 수의 스레드를 할당해주는 것이 좋다.
	- 이 설정값을 잡알 줄 때는 `log.dirs`에 지정된 로그 디렉토리별 스레드 수라는 점을 명심해야 된다.
		- 예시) `num.recovery.threads.per.data.dir`이 8이고 `log.dirs`에 지정된 경로 수가 3개일 경우, 전체 스레드 수는 24개가 된다.
- `auto.create.topics.enable`
	- 카프카 기본 설정에서는 아래와 같은 상황에서 브로커가 토픽을 자동으로 생성하도록 되어 있다.
		- 프로듀서가 토픽에 메시지를 쓰기 시작할 때
		- 컨슈머가 토픽으로부터 메시지를 읽기 시작할 때
		- 클라이언트가 토픽에 대한 메타데이터를 요청할 때
	- 토픽 자동 생성은 바람지갛지 않은 경우가 많다.
		- 특히 카프카에 토픽을 생성하지 않고 존재 여부만 확인할 방법이 없다는 점이 그렇다.
	- 만약 토픽 생성을 명시적으로 관리하고자 할 경우, `auto.create.topics.enable` 설정값을 `false`로 놓을 수 있다.
- `auto.leader.rebalance.enable`
	- 모든 토픽의 리더 역할이 하나의 브로커에 집중됨으로써 카프카 클러스터의 규ㅜㄴ형이 깨지는수가 있다.
	- 이 설정을 활성화해주면 가능한 한 리더의 역할이 균등하게 분산되도록 함으로써 이러한사태를 발생하는 것을방지할 수 있다.
	- 이 설정을 켜면파티션의 분포 상태를 주기적으로 확인하는 백그라운드 스레드가 시작된다.
		- 이 주기는 `leader.imbalance.check.interval.seconds`로 설정이 가능하다.
	- 만약 저네 파티션 중 특정 브로커에 리더 역할이 할당된 파티션 비율이 `leader.imbalance.per.broker.percentage`에 설정된 값을 넘어가면파티션의 선호 리더 리밸런싱이 발생한다.
- `delete.topic.enable`
	- 환경과 데이터 보존 가이드라인에 따라 클러스터의 토픽을 임의로 삭제하지 못하게끔 막아야 할 때가 있다.
	- 이 플래그를 flase로 잡아 주면 토픽 삭제 기능이 막힌다.

### 토픽별 기본값

- 카프카 브로커 설정은 새로 생성되는 토픽에 적용되는 설정의 기본값 역시 지정한다.
- `num.partitions`
	- 새로운 토픽이 생성될 떄 몇 개의 파티션을 갖게 되는지를 결정하며, 주로 자동 토픽생성 기능이 활성화되어 있을 때 사용된다.
	- 기본값은 1
	- 토픽의 파티션 개수는 늘릴 수만 있지 줄일 수는 없다.
	- 브로커가 추가될 때 클러스터 전체에 걸쳐 메시지 부하가 고르게 분산되도록 파티션 개수를 잡아 주는 게 중요하다.
	- 많은 사용자들은 토픽당 파티션 개수를 클러스터 내 브로커 수와 맞추거나 아니면 배수로 설정한다.
	- 파티션은 많아야 하지만 그렇다고 해서 너무 많으면 부하가 될 수 있다.
		- 너무 많은 파티션은 브로커의 메모리와 다른 자원들을 사용할 뿐만 아니라 메타데이터 업데이트나 리더 역할 변경에 걸리는 시간 역시 증가시킨다.
	- 만약 토픽의 목표 처리량과 컨슈머의 예상 처리량에 대해 어느 정도 추정값이 있다면 전자를 후자로 나눔으로써 필요한 파티션 수를 계산할 수 있다.
		- 예시) 주어진 토픽에 초당 1GB를 읽거나 쓰고자 하는데, 컨슈머 하나는 초당 50MB만 처리가 가능하다고 하면 최소한 20개의 파티션이 필요하다고 예측할 수 있는 것이다.
	- 만약 상세한 정보가 없다면, 경험상 매일 디스크 안에 저장되어 있는 파티션의 용량을 6GB 미만으로 유지하는 것이 대체로 결과가 좋았다.
- `default.replication.factor`
	- 자동 토픽 생성 기능이 활성화되어 있을 경우, 이 설정은 새로 생성되는 토픽의 복제 팩터를 결정한다.
	- 레플리카 셋 안에 일부러 정지시킨 레플리카와 예상치 않게 정지된 레플리카가 동시에 하나씩 발생해도 장애가 발생하지 않기위해, 최소한 3개의 레플리카를 가져야 한다.
- `log.retention.ms`
	- 카프카가 얼마나 오랫ㅎ동안 메시지를 보존해야 하는지를 지정할 떄 가장 많이 사용되는 설정이 시간 기준 보존 주기 설정이다.
	- 기본값은 `log.retntion.hours` 설정을 사용하면 168시간(1주일)이다.
	- 또 다른 단위로 `log.retention.munutes`나 `log.retention.ms`를 사용할 수도 있다.
	- 1개 이상의 설정이 정의되었을 경우 더 작은 단위 설정값이 우선권을 가진다.
- `log.retention.bytes`
	- 메시지 만료의 또 다른 기준은 보존되는 메시지의 용량이다.
	- 파티션 단위로 적용된다.
		- 만약 8개의 파티션을 가진 토픽에 `log.retention.bytes` 설정값이 1GB로 잡혀 있다면, 토픽의 최대 저장 용량은 8GB가 되는 것이다.
	- `log.retention.bytes` 설정과 `log.retention.ms` 설정을 둘 다 잡아 주었다면, 두 조건 중 하나의 조건만 성립해도 메시지가 삭제될 수 있다.
- `log.segment.bytes`
	- 로그 세그먼트의 크기가 `log.segment.bytes`에 지정된 크기에 다다르면, 브로커는 기존 로그 세그먼트를 다고 새로운 세그먼트를 연다.
	- 로그 세그먼트는 닫히기 전까지는 만료와 삭제의 대상이 되지 않는다.
	- 작은 로그 세그먼트 크기는 파일을 더 자주 닫고 새로 할당한다는 것이다.
	- 토픽에 메시지가 뜸하게 주어지는 상황에서는 로그 세그먼트의 크기를 조절해주는 것이 중요하다.
		- 예시) 토픽에 들어오는 메시지가 하루에 100MB인 상황에서 `log.segment.bytes`가 기본값으로 잡혀 있다면 세그먼트 하나를 채울 때까지 10일이 걸린다. 로그 세그먼트가 닫히기 전까지 메시지는 만료되지 않으므로 `log.retention.ms`가 1주일로 잡혀 있을 경우 닫힌 로그 세거믄터가 만료될 떄까지 실제로는 최대 17일치 메시지가 저장되어 있을 수 있다.
- `log.roll.ms`
	- 로그 세그먼트 파일이 닫히는 시점을 제어하는 또 다른 방법으로 파일이 닫혀야 할 때까지 기다리는 시간이다.
	- `log.retention.bytes`와 `log.roll.ms` 중 하나라도 도달한 경우 세그먼트를 닫는다.
- `min.insync.replicas`
	- 데이터 지속성 위주로 클러스터를 설정할 떄, `min.insync.replicas`를 2로 잡아주면 최소한 2개의 레플리카가 최신 상태로 프로듀서와 동기화되도록 할 수 있다.
	- 이것은 프로듀서의 ack 설정을 'all'로 잡아 주는 것과 함께 사용한다.
		- 이렇게 하면 프로듀서의 쓰기 작업이 성공하기 우해 최소한 두 개의 레플리카가 응답하도록 할 수 있다.
		- 이것은 아래와 같은 상황에서 데이터 유실을 방지할 수 있다.
			1. 리더가 쓰기 작업에 응답한다.
			2. 리더에 장애가 발생한다.
			3. 리더 역할이 최근의 쓰기 작업 내역을 복제하기 전의 다른 레플리카로 옮겨진다.
	- 지속성을 높이기 위해 이 값을 올려잡아 줄 경우 추가적인 오버헤드가 발생하면 성능이 떨어지는 부작용이 발생할 수 있다.
		- 따라서 몇 개의 메시지 유실 정도는 상관없고, 높은 처리량을 받아내야 하는 클러스터의 경우, 이 설정값을 기본값인 1에서 변경하지 않을 것을 권장한다.
- `message.max.bytes`
	- 카프카 브로커는 쓸 수 있는 메시지의 최대 크기를 제한한다.
	- 기본값은 1MB이다.
	- 프로듀서가 여기에 지정된 값보다 더 큰 크기의 메시지를 보내려고 시도하면 브로커는 메시지를 거부하고 에러를 리턴할 것이다.
	- 카프카 브로커에 설정되는 메시지 크기는 컨슈머 클라이언트의 `fetch.message.max.bytes` 설정과 맞아야 한다.
